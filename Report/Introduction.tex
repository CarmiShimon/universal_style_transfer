\begin{intro}
	Style transfer aims to synthesize an image that
	preserves some notion of the content but carries characteristics of the style. The key challenge is how to extract effective representations of the style and then match it in the content image.
	Transferring the style from one image onto another can be considered a problem of texture transfer. In texture transfer the goal is to synthesize a texture from a source image while constraining the texture synthesis in order to preserve the semantic content of a target image. For texture synthesis there exist a large range of powerful non-parametric algorithms that can synthesise photo realistic natural textures by re-sampling the pixels of a given source texture [1, 2, 3, 4]. Most previous texture transfer algorithms rely on these non-parametric methods for texture synthesis while using different ways to preserve the structure of the target image. For instance, Efros and Freeman introduce a correspondence map that includes features of the target image such as image intensity to constrain the texture synthesis procedure[3]. Lee et al. improve this algorithm by additionally informing the texture transfer with edge orientation information[5]. Although these algorithms achieve remarkable results, they all suffer from the same fundamental limitation: they use only low-level image features of the target image to inform the texture transfer. Ideally, however, a style transfer algorithm should be able to extract the semantic image content (e.g. the objects and the general scenery) and then inform a texture transfer procedure to render the semantic content of the target image (content and style image) in the style of the style image. Therefore, a fundamental prerequisite is to find image representations that independently model variations in the semantic image content and the style in which it is presented.
	To generally separate content from style in natural images is still an extremely difficult problem.\newline
	However, the recent advance of Deep Convolutional Neural Networks[6] has produced powerful computer vision systems that learn to extract high-level semantic information from natural images.  It was shown that Convolutional Neural Networks trained with sufficient labeled data on specific tasks such as object recognition learn to extract high-level image content in generic feature representations that generalize across data sets[7] and even to other visual information processing tasks, including texture recognition[8] and artistic style classification[15].\newline
	\hspace{10mm} In this work we show how the generic feature representations learned by high-performing Convolutional Neural Networks (Encoder) followed by efficient feature Whitening-Coloring  transforms and a compatible reconstruction (Decoder) can be used to manipulate the content and the style of natural images.
	Furthermore, 
\end{intro}