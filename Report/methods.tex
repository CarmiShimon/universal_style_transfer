\hspace{0.5cm} Our project contains the following tasks: 
\begin{itemize}
	\item Building and training the models required for Universal Style Transfer via Feature Transforms as described in \cite{bib11}.
	\item Implementing a PyTorch program to perform the UST algorithm as described in \cite{bib11}.
	\item Present innovative method to enhance the style transfer effect. This method shall be referred to as \textit{boosting}.
	\item In \cite{bib11}, Li et al present a method to transfer style from a pair of style images, in such a way that the result will contain stylistic aspects of both. Let's refer to this problem as \textit{Merge}-UST. In this task we research methods that utilize the UST framework to perform Merge-UST in a computationally lighter fashion than presented in \cite{bib11}.
\end{itemize}


\subsection{Models and Training}
\label{models_methods_lbl}
\input{models_methods}

\subsection{Stylization}\label{sec:method_stylization}
\textcolor{red}{TODO}: The UST algorithm formulates style transfer as an image reconstruction process coupled with feature transformation of whitening and coloring. The reconstruction step is responsible for inverting features back to the RGB space and the feature transformation matches the feature statistics of a content image to a style image.
\textbf{WCT}. The WCT \cite{bib11} formulates stylization as an image reconstruction problem with feature projections. To utilize WCT, an auto-encoder for general image reconstruction is first trained. We used the VGG-19 model \cite{bib20} as the encoder $\varepsilon$(weights are kept fixed) and trains a decoder D for reconstructing the input image. The decoder is symmetrical to the encoder and uses up-sampling layers to enlarge the spatial resolutions of the feature maps, (see figure ~\ref{fig:full-pipeline}). Once the auto-encoder is trained, a pair of projection functions are inserted at the network bottleneck to perform stylization through the whitening ($P_C$) and coloring ($P_S$) transforms. The key idea behind the WCT is to directly match feature correlations of the content image to those of the style image via the two projections. Specifically, given a pair of content image $I_C$ and style image $I_S$, the WCT first extracts their vectorised VGG features $C_f=\varepsilon(I_C)$ and $S_f=\varepsilon(I_S)$, and then transform the content feature $C_f$ via
\begin{equation}
CS_f = P_SP_CH_C
\end{equation}
Where $P_C=E_C\Lambda_C^{-\frac{1}{2}}$, and $P_S=E_S\Lambda_S^{\frac{1}{2}}$. Here $\Lambda_C$ and $\Lambda_S$ are the diagonal matrices with the eigenvalues of the covariance matrix $C_fC_f^T$ and $S_fS_f^T$ respectively. The matrices $E_C$ and $E_S$ are the corresponding orthonormal matrices of the eigenvalues, respectively, (see figure ~\ref{fig:WCT}). After the transformation, the correlations of
transformed features match those of the style features, i.e., $CS_fCS_f^T=S_fS_f^T$. Finally, the stylized image is obtained by directly feeding the transformed feature
map into the decoder: $Y = D(CS_f)$. For better stylization performance, Li et
al. \cite{bib11} use a multi-level stylization strategy, which performs the WCT on the
VGG features at different layers.
The WCT performs well for artistic image stylization. However it generates
structural artifacts (e.g., distortions on object boundaries)
%%% whitening-coloring figures %%%
\begin{figure}[h!]
	\centering
	\begin{subfigure}[b]{0.4\linewidth}
	\includegraphics[width=\linewidth]{whitening.png}
		\caption{Data whitening}
		\end{subfigure}
	\begin{subfigure}[b]{0.4\linewidth}
	\includegraphics[width=\linewidth]{coloring.png}
	\caption{Data coloring}
	\end{subfigure}
	\caption{Whitening-Coloring-Transformation}
	\label{fig:WCT}
\end{figure}


\subsection{Boost}
\label{boost_methods_lbl}
\input{boost_methods}

\subsection{Merge}
\label{merge_methods_lbl}
\input{merge_methods}
