\hspace{1cm} Style transfer aims to synthesize an image that
preserves some notion of the content but carries characteristics of the style. The key challenge is how to extract effective representations of the style and then match it in the content image.
Transferring the style from one image onto another can be considered a problem of texture transfer. In texture transfer the goal is to synthesize a texture from a source image while constraining the texture synthesis in order to preserve the semantic content of a target image. For texture synthesis there exist a large range of powerful non-parametric algorithms that can synthesise photo realistic natural textures by re-sampling the pixels of a given source texture \cite{bib1, bib2, bib3, bib4}. Most previous texture transfer algorithms rely on these non-parametric methods for texture synthesis while using different ways to preserve the structure of the target image. For instance, Efros and Freeman introduce a correspondence map that includes features of the target image such as image intensity to constrain the texture synthesis procedure \cite{bib3}. Lee et al. improve this algorithm by additionally informing the texture transfer with edge orientation information \cite{bib3}. Although these algorithms achieve remarkable results, they all suffer from the same fundamental limitation: they use only low-level image features of the target image to inform the texture transfer. Ideally, however, a style transfer algorithm should be able to extract the semantic image content (e.g. the objects and the general scenery) and then inform a texture transfer procedure to render the semantic content of the target image (content and style image) in the style of the style image. Therefore, a fundamental prerequisite is to find image representations that independently model variations in the semantic image content and the style in which it is presented.
To generally separate content from style in natural images is still an extremely difficult problem.\newline
However, the recent advance of Deep Convolutional Neural Networks (CNNs)[6] has produced powerful computer vision systems that learn to extract high-level semantic information from natural images.  It was shown that CNNs trained with sufficient labeled data on specific tasks such as object recognition learn to extract high-level image content in generic feature representations that generalize across data sets \cite{bib3} and even to other visual information processing tasks, including texture recognition[8] and artistic style classification \cite{bib15}.\newline
\\
The main issue is how to properly and effectively apply the extracted style characteristics (feature correlations) to content images in a style-agnostic manner.\newline
\\
In this work we show how the generic feature representations learned by high-performing CNNs (Encoder) followed by efficient feature Whitening-Coloring transforms (WCTs) and a compatible reconstruction (Decoder) can be used to manipulate the content and the style of natural images.
We introduce a novel methods which boosts style transfer by taking advantage of the existence of feature representations from state-of-the-art CNNs.  
We also show new and efficient methods of combining different style images into the target image (content image) by using WCT algorithm efficiently.
Our goal was to invent new and efficient ways of UST based on the work by Li et al. \cite{bib11}.
Our method consists of a stylization step and a smoothing step. Both have a closed-form solution1and can be computed efficiently. The stylization step is based on the (WCT) \cite{bib10}, which stylizes images via feature projections. The WCT was designed for artistic stylization.
Our results show similar results as presented in \cite{bib10} while showing efficiency in computation.